{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p4w36BJmBpYC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let use build our model"
      ],
      "metadata": {
        "id": "ZXS9MWx5Bvh9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First we need to create a new class- (class name can be of our choice)\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, input, output):\n",
        "        super().__init__()\n",
        "        # now this is where we start building our main layers\n",
        "        # One thing we can do here is assign our layers to variables\n",
        "\n",
        "        # This is our convolutional layer\n",
        "        self.conv1 = nn.Conv2d(input, 96, kernel_size= 11, stride=4, padding=0)\n",
        "        # here first we give our input value, then output, kernel size, stride value, and lastly padding value\n",
        "        # we will follow the same thing in the next layer as well\n",
        "\n",
        "\n",
        "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\n",
        "\n",
        "        # now lets build our fully connected layers\n",
        "        # the settings are the same (kinda)\n",
        "        self.fcn1= nn.Linear(256 * 6 * 6, 4096)\n",
        "        self.fcn2 = nn.Linear(4096, 4096)\n",
        "        self.fcn3 = nn.Linear(4096, output)\n",
        "\n",
        "\n",
        "        # now lets build activation, pooling, and flatten layers\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.maxpooling = nn.MaxPool2d(kernel_size=3, stride =2)\n",
        "        self.norm = nn.LocalResponseNorm(size=5, k=2)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self._init_weights()\n",
        "\n",
        "    # now lets build our forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.maxpooling(self.norm(self.activation(self.conv1(x))))\n",
        "        x = self.maxpooling(self.norm(self.activation(self.conv2(x))))\n",
        "        x = (self.activation(self.conv3(x)))\n",
        "        x = (self.activation(self.conv4(x)))\n",
        "        x = self.maxpooling(self.activation(self.conv5(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.activation(self.fcn1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.activation(self.fcn2(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fcn3(x)\n",
        "\n",
        "    # def weight_init(self):\n",
        "    #     print(next(self.modules()))\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for i, layer in enumerate(self.modules()):\n",
        "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "                if layer.bias is not None:\n",
        "                    if i in [1,2,3,4,5,6,7]:  # manual bias 1 if needed\n",
        "                        nn.init.constant_(layer.bias, val=1)\n",
        "                    else:\n",
        "                        nn.init.constant_(layer.bias, val=0)\n",
        "\n",
        "model = AlexNet(input=3, output=1000)\n",
        "print(model)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     model = AlexNet(input=3, output=1000)\n",
        "#     print(model(torch.rand(1,3,227,227)).shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X3U14NDtk_J",
        "outputId": "7c7478ca-72a2-4d18-e310-a3454dbc5c0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (conv1): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
            "  (conv2): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (conv3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv5): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fcn1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "  (fcn2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (fcn3): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (maxpooling): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (norm): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
            "  (activation): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If-NQFa307v-",
        "outputId": "3ed314f0-6fc5-402c-bf26-43cea1d33f8c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "model = AlexNet(input=3, output=1000)\n",
        "summary(model, input_size=(1, 3, 227, 227))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uFKaLkwbP0X",
        "outputId": "90e31d95-6f94-4d59-c4a2-ba26b831afaa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "AlexNet                                  [1, 1000]                 --\n",
              "├─Conv2d: 1-1                            [1, 96, 55, 55]           34,944\n",
              "├─ReLU: 1-2                              [1, 96, 55, 55]           --\n",
              "├─LocalResponseNorm: 1-3                 [1, 96, 55, 55]           --\n",
              "├─MaxPool2d: 1-4                         [1, 96, 27, 27]           --\n",
              "├─Conv2d: 1-5                            [1, 256, 27, 27]          614,656\n",
              "├─ReLU: 1-6                              [1, 256, 27, 27]          --\n",
              "├─LocalResponseNorm: 1-7                 [1, 256, 27, 27]          --\n",
              "├─MaxPool2d: 1-8                         [1, 256, 13, 13]          --\n",
              "├─Conv2d: 1-9                            [1, 384, 13, 13]          885,120\n",
              "├─ReLU: 1-10                             [1, 384, 13, 13]          --\n",
              "├─Conv2d: 1-11                           [1, 384, 13, 13]          1,327,488\n",
              "├─ReLU: 1-12                             [1, 384, 13, 13]          --\n",
              "├─Conv2d: 1-13                           [1, 256, 13, 13]          884,992\n",
              "├─ReLU: 1-14                             [1, 256, 13, 13]          --\n",
              "├─MaxPool2d: 1-15                        [1, 256, 6, 6]            --\n",
              "├─Flatten: 1-16                          [1, 9216]                 --\n",
              "├─Linear: 1-17                           [1, 4096]                 37,752,832\n",
              "├─ReLU: 1-18                             [1, 4096]                 --\n",
              "├─Dropout: 1-19                          [1, 4096]                 --\n",
              "├─Linear: 1-20                           [1, 4096]                 16,781,312\n",
              "├─ReLU: 1-21                             [1, 4096]                 --\n",
              "├─Dropout: 1-22                          [1, 4096]                 --\n",
              "├─Linear: 1-23                           [1, 1000]                 4,097,000\n",
              "==========================================================================================\n",
              "Total params: 62,378,344\n",
              "Trainable params: 62,378,344\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 1.14\n",
              "==========================================================================================\n",
              "Input size (MB): 0.62\n",
              "Forward/backward pass size (MB): 5.27\n",
              "Params size (MB): 249.51\n",
              "Estimated Total Size (MB): 255.41\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsbgTYHMDj8d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}